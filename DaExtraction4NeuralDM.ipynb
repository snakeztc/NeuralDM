{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from swda import Transcript\n",
    "from swda import CorpusReader\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from nltk import FreqDist\n",
    "from nltk import word_tokenize\n",
    "corpus = CorpusReader('swda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the tag description\n",
    "tag_detail = {}\n",
    "with open('./swda/tag_description.csv', 'r') as csv_file:\n",
    "    tag_reader = csv.reader(csv_file)\n",
    "    for row in tag_reader:\n",
    "        tag_detail[row[1]] = (row[0], row[2])\n",
    "        \n",
    "tag_map = {}\n",
    "with open('./swda/tag_mapping.csv', 'r') as csv_file:\n",
    "    tag_reader = csv.reader(csv_file)\n",
    "    for row in tag_reader:\n",
    "        tag_map[row[1]] = row[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, tag_list, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(tag_list))\n",
    "    plt.xticks(tick_marks, tag_list, rotation=45)\n",
    "    plt.yticks(tick_marks, tag_list)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def norm_tokens(tokens):\n",
    "    result = []\n",
    "    for t in tokens:\n",
    "        if not ('[' in t or ']' in t or\n",
    "               '{' in t or '}' in t or \n",
    "               '+' in t or '/' in t or \n",
    "               '--' in t or \"#\" in t):\n",
    "            result.append(t)\n",
    "    return result\n",
    "    \n",
    "def should_append(utt):\n",
    "    if utt.damsl_act_tag() == '+':\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def norm_label(utt):\n",
    "    label = utt.damsl_act_tag()\n",
    "    if (label.startswith(\"fo\")):\n",
    "        label = 'fo_o_fw_by_bc'\n",
    "    #return tag_map[label]\n",
    "    return label\n",
    "\n",
    "def remove_tags(utt):\n",
    "    # remove tags\n",
    "    result = re.sub(r\"<(.*?)>\", \"\", utt)\n",
    "    # remove multiple space\n",
    "    result = re.sub(r\" +\", \" \", result)\n",
    "    result = re.sub(r\"([\\w/'+$\\s-]+|[^\\w/'+$\\s-]+)\\s*\", r\"\\1 \", result)\n",
    "    result = result.strip()\n",
    "    return result\n",
    "\n",
    "def get_vocab(utts):\n",
    "    vocab = []\n",
    "    for utt in utts:\n",
    "        vocab.extend([w for w in utt.split()])\n",
    "    return set(vocab)\n",
    "\n",
    "def get_words(utts):\n",
    "    words = []\n",
    "    for utt in utts:\n",
    "        words.extend([w for w in utt.split()])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_utts = []\n",
    "all_targets = []\n",
    "all_callers = []\n",
    "tag_set = set()\n",
    "cnt = 0;\n",
    "last_idx = {'A':-1, 'B':-1}\n",
    "black_list = []\n",
    "\n",
    "for utt in corpus.iter_utterances(display_progress=False):\n",
    "    tokens = utt.text_words()\n",
    "    caller = utt.caller\n",
    "    label = norm_label(utt)\n",
    "    b_should_append = should_append(utt)\n",
    "    \n",
    "    # check for merging\n",
    "    if b_should_append:\n",
    "        idx = last_idx.get(caller)\n",
    "        if idx >= 0:\n",
    "            all_utts[idx] = all_utts[idx] + ' ' + ' '.join(norm_tokens(tokens))\n",
    "            continue\n",
    "        else:\n",
    "            print \"ERROR\"\n",
    "            break\n",
    "    \n",
    "    # check if empty\n",
    "    norm_text = remove_tags(' '.join(norm_tokens(tokens)))\n",
    "    if not norm_text:\n",
    "        if label == 'x' and utt.text:\n",
    "            norm_text = utt.text\n",
    "        else:       \n",
    "            continue\n",
    "        \n",
    "    # update previous speaker utt\n",
    "    last_idx[caller] = cnt\n",
    "    \n",
    "    # save\n",
    "    all_utts.append(norm_text)\n",
    "    all_targets.append(label)\n",
    "    all_callers.append(caller)\n",
    "    tag_set.add(label)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10062\n",
      "2023\n",
      "390\n",
      "4\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "all_yes = [utt for utt, tag in zip(all_utts, all_targets) if tag in [\"aa\", \"ny\" \"na\"] and \"no\" not in utt.lower()]\n",
    "print len(all_yes)\n",
    "all_no = [utt for utt, tag in zip(all_utts, all_targets) if tag in [\"nn\", \"ar\", \"ng\"]]\n",
    "print len(all_no)\n",
    "all_unknown = [utt for utt, tag in zip(all_utts, all_targets) if tag in [\"aap_am\", \"no\"]]\n",
    "print len(all_unknown)\n",
    "all_correct = ['correct', \"you won\", \"yeah that's who I am thinking\", \"your guess is correct\"]\n",
    "print len(all_correct)\n",
    "all_wrong = [\"wrong\", \"incorrect\", \"your guess is wrong\", \"that's not who I am thinking\"]\n",
    "print len(all_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10062\n",
      "2023\n",
      "390\n"
     ]
    }
   ],
   "source": [
    "#all_yes = list(np.random.choice(all_yes, 100))\n",
    "print len(all_yes)\n",
    "#all_no = list(np.random.choice(all_no, 100))\n",
    "print len(all_no)\n",
    "#all_unknown = list(np.random.choice(all_unknown, 100))\n",
    "print len(all_unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1460\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(',', 7822),\n",
       " ('.', 6405),\n",
       " ('Yeah', 2872),\n",
       " ('I', 1222),\n",
       " ('right', 1024),\n",
       " ('No', 996),\n",
       " ('yeah', 972),\n",
       " ('Right', 915),\n",
       " (\"That's\", 871),\n",
       " ('true', 638)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get vocabular size\n",
    "vocab = get_vocab(all_yes).union(get_vocab(all_no).union(get_vocab(all_unknown).union(get_vocab(all_correct).union(get_vocab(all_wrong)))))\n",
    "print len(vocab)\n",
    "all_words = get_words(all_yes) + get_words(all_no) + get_words(all_unknown) + get_words(all_correct) + get_words(all_wrong)\n",
    "vocab_dist = FreqDist(all_words)\n",
    "vocab_dist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct Classificaiton to make sure uni/bigram is good enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12483\n",
      "12483\n"
     ]
    }
   ],
   "source": [
    "order_utts = all_yes + all_no + all_unknown + all_correct + all_wrong\n",
    "order_labels = [0] * len(all_yes) + [1] * len(all_no) + [2] * len(all_unknown) + [3] * len(all_correct) + [4] * len(all_wrong)\n",
    "random_idx = np.random.permutation(len(order_utts))\n",
    "\n",
    "utts = [order_utts[idx] for idx in random_idx]\n",
    "labels = np.array([order_labels[idx] for idx in random_idx])\n",
    "print len(utts)\n",
    "print len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12483, 5415)\n"
     ]
    }
   ],
   "source": [
    "bigram = CountVectorizer(min_df=0, ngram_range=(1,2), lowercase=False)\n",
    "unigram = CountVectorizer(min_df=0, ngram_range=(1,1), lowercase=False)\n",
    "train_utts = bigram.fit(utts)\n",
    "unigram.fit(utts)\n",
    "ngram_train = bigram.transform(utts)\n",
    "print ngram_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clf = SVC(C=1.0, kernel='rbf', gamma=10.0).fit(ngram_train, labels)\n",
    "clf = LinearSVC(C=1.0).fit(ngram_train, labels)\n",
    "predicted = clf.predict(ngram_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     10062\n",
      "          1       0.98      0.99      0.98      2023\n",
      "          2       0.99      0.93      0.96       390\n",
      "          3       1.00      1.00      1.00         4\n",
      "          4       1.00      1.00      1.00         4\n",
      "\n",
      "avg / total       0.99      0.99      0.99     12483\n",
      "\n",
      "[[10014    46     2     0     0]\n",
      " [   17  2004     2     0     0]\n",
      " [   26     3   361     0     0]\n",
      " [    0     0     0     4     0]\n",
      " [    0     0     0     0     4]]\n"
     ]
    }
   ],
   "source": [
    "print metrics.classification_report(labels, predicted)\n",
    "print metrics.confusion_matrix(labels, predicted)\n",
    "#plot_confusion_matrix(cm=metrics.confusion_matrix(labels, predicted), tag_list=['yes','no', 'unkown'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Huh-uh .', 0, 1)\n",
      "(\"I don't think so .\", 0, 1)\n",
      "('Huh-uh', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('I guess so  .', 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "(\"Um , that's probably true .\", 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Well , yes ,', 1, 0)\n",
      "(\"That's probably true .\", 2, 0)\n",
      "('I guess so .', 2, 0)\n",
      "(\"that's probably true .\", 2, 0)\n",
      "('Huh-uh ,', 0, 1)\n",
      "('Yes ,', 1, 0)\n",
      "('they would .', 1, 0)\n",
      "('uh-huh ,', 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Yeah ,', 2, 0)\n",
      "('huh-uh .', 0, 1)\n",
      "(\"That's probably right .\", 2, 0)\n",
      "(\"I don't know ,\", 1, 2)\n",
      "('huh-uh .', 0, 1)\n",
      "(\"I don't ,\", 2, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('uh-huh ,', 1, 0)\n",
      "('Could be .', 2, 0)\n",
      "('Huh-uh ,', 0, 1)\n",
      "(\"I don't think ,\", 0, 1)\n",
      "('I suppose .', 2, 0)\n",
      "(\"I don't think so .\", 0, 1)\n",
      "('Um .', 2, 0)\n",
      "('I think so .', 2, 0)\n",
      "('I , I , I guess so .', 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('huh-uh .', 0, 1)\n",
      "('N- , -', 1, 0)\n",
      "(\"I don't think so .\", 2, 1)\n",
      "('I know ,', 1, 2)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Uh-huh .', 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('huh-uh .', 0, 1)\n",
      "('That could be .', 0, 2)\n",
      "('I guess so .', 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Yeah .', 2, 0)\n",
      "(\"It isn't .\", 0, 1)\n",
      "('I guess ,', 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Huh-uh ,', 0, 1)\n",
      "(\"We don't , -\", 0, 1)\n",
      "('Probably .', 2, 0)\n",
      "('huh-uh .', 0, 1)\n",
      "('Uh-uh .', 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Uh-huh .', 1, 0)\n",
      "(\"I wouldn't think ,\", 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Sometimes ,', 0, 2)\n",
      "(\"I don't .\", 2, 1)\n",
      "('Uh-huh ,', 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Huh .', 2, 0)\n",
      "('I guess so .', 2, 0)\n",
      "('huh-uh .', 0, 1)\n",
      "(\"I really don't .\", 2, 0)\n",
      "(\"I don't think so .\", 0, 1)\n",
      "(\"I don't either .\", 2, 0)\n",
      "('Probably ,', 2, 0)\n",
      "('uh-uh .', 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "(\"We don't either .\", 1, 0)\n",
      "('I guess ,', 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "(\"I don't ,\", 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('huh-uh ,', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Uh , yeah .', 2, 0)\n",
      "('Huh-uh ,', 0, 1)\n",
      "('Oh , yeah ,', 1, 0)\n",
      "('huh-uh .', 0, 1)\n",
      "(\"that's probably true .\", 2, 0)\n",
      "('N- ,', 1, 0)\n",
      "('huh-uh .', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('Huh-uh .', 0, 1)\n",
      "('I guess so .', 2, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('I guess .', 2, 0)\n",
      "('N- , -', 1, 0)\n",
      "('Huh-uh .', 0, 1)\n",
      "('I do .', 1, 0)\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for idx in range(len(labels)):\n",
    "    if not labels[idx] == predicted[idx]:\n",
    "        print (utts[idx], labels[idx], predicted[idx])\n",
    "        cnt += 1\n",
    "print cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the feature as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bigram_yes = bigram.transform(all_yes)\n",
    "bigram_no = bigram.transform(all_no)\n",
    "bigram_unkown = bigram.transform(all_unknown)\n",
    "bigram_correct = bigram.transform(all_correct)\n",
    "bigram_wrong = bigram.transform(all_wrong)\n",
    "\n",
    "unigram_yes = unigram.transform(all_yes)\n",
    "unigram_no = unigram.transform(all_no)\n",
    "unigram_unkown = unigram.transform(all_unknown)\n",
    "unigram_correct = unigram.transform(all_correct)\n",
    "unigram_wrong = unigram.transform(all_wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl.dump({\"yes\":bigram_yes, \"no\": bigram_no, \"unknown\": bigram_unkown, \"correct\": bigram_correct, \"wrong\": bigram_wrong}, open('bigram_usr_resp.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pkl.dump({\"yes\":unigram_yes, \"no\": unigram_no, \"unknown\": unigram_unkown, \"correct\": unigram_correct, \"wrong\": unigram_wrong}, open('unigram_usr_resp.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pkl.dump({\"yes\":all_yes, \"no\": all_no, \"unknown\": all_unknown, \"correct\": all_correct, \"wrong\": all_wrong}, open('utts.pkl', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
